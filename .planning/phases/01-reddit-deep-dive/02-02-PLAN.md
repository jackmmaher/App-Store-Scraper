---
phase: 01-reddit-deep-dive
plan: 04
type: execute
---

<objective>
Create the analyze and solutions API endpoints.

Purpose: Complete the backend API surface for Reddit Deep Dive.
Output: Working `/api/reddit/analyze` and `/api/reddit/solutions` endpoints.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./02-02-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-reddit-deep-dive/02-01-SUMMARY.md
@docs/plans/2026-02-01-reddit-deep-dive-design.md

Relevant source files:
@lib/reddit/types.ts
@lib/reddit/analyzer.ts
@lib/supabase.ts
@lib/crawl/orchestrator.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create analyze API endpoint</name>
  <files>app/api/reddit/analyze/route.ts</files>
  <action>
Create POST endpoint that orchestrates the full analysis flow:

Input: `RedditSearchConfig`
Output: `RedditAnalysisResult`

Flow:
1. Validate authentication and input
2. Call crawl-service `/crawl/reddit/deep-dive` with config
3. Pass results to `analyzeRedditData()`
4. Store results in database via `createRedditAnalysis()`
5. Link to competitor via `linkRedditAnalysisToCompetitor()`
6. Return full `RedditAnalysisResult`

Use existing crawl orchestrator pattern for calling crawl-service:
```typescript
const crawlServiceUrl = process.env.CRAWL_SERVICE_URL || 'http://localhost:8000';
const response = await fetch(`${crawlServiceUrl}/crawl/reddit/deep-dive`, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'X-API-Key': process.env.CRAWL_SERVICE_API_KEY || '',
  },
  body: JSON.stringify({
    search_topics: config.searchTopics,
    subreddits: config.subreddits,
    time_filter: config.timeRange,
  }),
});
```

Handle timeouts (5 minute max for Reddit scraping). Stream progress if possible, or return 202 Accepted with job ID for long operations.

For v1, use synchronous approach with generous timeout. Add streaming later if needed.
  </action>
  <verify>
Test with curl:
```bash
curl -X POST http://localhost:3000/api/reddit/analyze \
  -H "Content-Type: application/json" \
  -d '{"competitorId": "...", "problemDomain": "...", "searchTopics": ["..."], "subreddits": ["..."], "timeRange": "month"}' \
  --cookie "auth-token=..."
```
Returns 200 with RedditAnalysisResult
  </verify>
  <done>Analyze endpoint works end-to-end: scrape → analyze → store → return</done>
</task>

<task type="auto">
  <name>Task 2: Create get analysis endpoint</name>
  <files>app/api/reddit/analysis/[competitorId]/route.ts</files>
  <action>
Create GET endpoint to fetch existing analysis for a competitor:

```typescript
// GET /api/reddit/analysis/[competitorId]
export async function GET(
  request: NextRequest,
  { params }: { params: Promise<{ competitorId: string }> }
)
```

1. Validate authentication
2. Call `getRedditAnalysis(competitorId)`
3. If found, also fetch solution annotations via `getUnmetNeedSolutions(analysisId)`
4. Merge solutions into unmetNeeds array
5. Return full result or null if no analysis exists

Return 200 with data, or 404 if no analysis found.
  </action>
  <verify>
Test with curl:
```bash
curl http://localhost:3000/api/reddit/analysis/COMPETITOR_ID \
  --cookie "auth-token=..."
```
Returns analysis if exists, 404 if not
  </verify>
  <done>Get endpoint returns existing analysis with merged solution annotations</done>
</task>

<task type="auto">
  <name>Task 3: Create solutions API endpoint</name>
  <files>app/api/reddit/solutions/route.ts</files>
  <action>
Create PUT endpoint to save user solution annotations:

Input:
```typescript
{
  analysisId: string;
  solutions: Array<{ needId: string; notes: string }>;
}
```

Output: `{ success: boolean }`

1. Validate authentication and input
2. Call `saveUnmetNeedSolutions(analysisId, solutions)`
3. Return success/failure

Use upsert pattern in the Supabase function - if solution exists, update it; otherwise insert.
  </action>
  <verify>
Test with curl:
```bash
curl -X PUT http://localhost:3000/api/reddit/solutions \
  -H "Content-Type: application/json" \
  -d '{"analysisId": "...", "solutions": [{"needId": "need-1", "notes": "Our approach..."}]}' \
  --cookie "auth-token=..."
```
Returns 200 with success: true
  </verify>
  <done>Solutions endpoint saves annotations, retrievable via get analysis endpoint</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `npm run build` succeeds without errors
- [ ] All three endpoints respond correctly
- [ ] Data persists to database
- [ ] Solution annotations merge correctly when fetching analysis
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Full backend API working: generate-config → analyze → get analysis → save solutions
</success_criteria>

<output>
After completion, create `.planning/phases/01-reddit-deep-dive/02-02-SUMMARY.md`
</output>
