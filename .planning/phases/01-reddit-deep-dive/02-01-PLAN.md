---
phase: 01-reddit-deep-dive
plan: 03
type: execute
---

<objective>
Extend Reddit crawler for deep scraping and implement AI semantic analysis.

Purpose: Enable comprehensive Reddit data collection and intelligent extraction of unmet needs.
Output: Working `/api/reddit/analyze` endpoint that scrapes Reddit and returns structured analysis.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./02-01-SUMMARY.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-reddit-deep-dive/01-01-SUMMARY.md
@.planning/phases/01-reddit-deep-dive/01-02-SUMMARY.md
@docs/plans/2026-02-01-reddit-deep-dive-design.md

Relevant source files:
@crawl-service/crawlers/reddit.py
@lib/reddit/types.ts
@lib/reddit/config-generator.ts
@lib/supabase.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend Reddit crawler for deep scraping</name>
  <files>crawl-service/crawlers/reddit.py</files>
  <action>
Extend RedditCrawler class with a new method `crawl_deep_dive`:

```python
async def crawl_deep_dive(
    self,
    search_topics: List[str],
    subreddits: List[str],
    time_filter: str = "month",  # week, month, year
    max_posts_per_combo: int = 50,
    max_comments_per_post: int = 30,
) -> dict:
    """
    Deep dive scraping for Reddit analysis.
    Searches each topic in each subreddit, fetches comments on high-engagement posts.
    Returns structured data for AI analysis.
    """
```

Implementation:
1. For each subreddit + topic combination:
   - Search: `reddit.com/r/{sub}/search.json?q={topic}&restrict_sr=on&sort=relevance&limit={max}&t={time_filter}`
   - Collect posts with: id, title, selftext, score, num_comments, created_utc, subreddit, permalink
2. Filter for posts with score > 5 or num_comments > 3 (engagement threshold)
3. Fetch comments for top 20 high-engagement posts (score > 20 or comments > 10)
4. Deduplicate posts by ID across searches
5. Rate limit: 1.5s between requests, use existing REDDIT_HEADERS
6. Return:
```python
{
    "posts": [...],  # All posts with comments
    "stats": {
        "total_posts": int,
        "total_comments": int,
        "subreddits_searched": List[str],
        "topics_searched": List[str],
        "date_range": {"start": str, "end": str}
    }
}
```

Handle rate limiting gracefully - if 429 response, wait 60s and retry once.
  </action>
  <verify>
Test manually:
```python
crawler = RedditCrawler()
result = await crawler.crawl_deep_dive(
    search_topics=["quit vaping", "nicotine cravings"],
    subreddits=["QuitVaping", "stopsmoking"],
    time_filter="month"
)
assert len(result["posts"]) > 0
```
  </verify>
  <done>crawl_deep_dive method works, returns posts with comments, handles rate limits</done>
</task>

<task type="auto">
  <name>Task 2: Add FastAPI endpoint for deep dive crawling</name>
  <files>crawl-service/main.py</files>
  <action>
Add new endpoint to FastAPI service:

```python
@app.post("/crawl/reddit/deep-dive")
async def crawl_reddit_deep_dive(request: RedditDeepDiveRequest) -> RedditDeepDiveResponse:
    """
    Deep dive Reddit scraping for semantic analysis.
    """
```

Request model:
```python
class RedditDeepDiveRequest(BaseModel):
    search_topics: List[str]
    subreddits: List[str]
    time_filter: str = "month"  # week, month, year
    max_posts_per_combo: int = 50
    max_comments_per_post: int = 30
```

Response model:
```python
class RedditDeepDiveResponse(BaseModel):
    posts: List[dict]
    stats: dict
    success: bool
    error: Optional[str] = None
```

Use existing auth pattern (API key in header). Handle errors gracefully.
  </action>
  <verify>
Test with curl when crawl-service is running:
```bash
curl -X POST http://localhost:8000/crawl/reddit/deep-dive \
  -H "Content-Type: application/json" \
  -H "X-API-Key: $CRAWL_SERVICE_API_KEY" \
  -d '{"search_topics": ["test"], "subreddits": ["test"], "time_filter": "week"}'
```
  </verify>
  <done>FastAPI endpoint works, returns structured response</done>
</task>

<task type="auto">
  <name>Task 3: Create AI analyzer for semantic extraction</name>
  <files>lib/reddit/analyzer.ts</files>
  <action>
Create the AI semantic analyzer that processes Reddit data:

```typescript
export async function analyzeRedditData(
  posts: RedditPost[],
  stats: RedditStats,
  problemDomain: string
): Promise<{
  unmetNeeds: UnmetNeed[];
  trends: TrendAnalysis;
  sentiment: SentimentBreakdown;
  languagePatterns: string[];
  topSubreddits: SubredditSummary[];
}>
```

Implementation:
1. Format posts/comments for Claude prompt (limit to ~50 posts, prioritize by engagement)
2. Claude prompt objectives:
   - Identify top 5-7 unmet needs (problems people mention lacking solutions for)
   - Categorize by severity (High/Medium/Low) based on frequency + emotional intensity
   - Extract evidence: post count mentioning each need, avg engagement, representative quotes
   - Analyze sentiment breakdown (% frustrated, seeking help, success stories)
   - Identify language patterns ("I wish there was...", "Has anyone tried...")
3. Parse Claude response into structured types
4. Calculate trend metrics from post timestamps (recent vs older volume)
5. Aggregate subreddit stats

Claude prompt should return JSON for easy parsing. Use model: claude-sonnet-4-20250514, max_tokens: 4000.

Handle API errors gracefully - return partial results if possible.
  </action>
  <verify>TypeScript compiles. Manual test with sample Reddit data returns valid analysis structure.</verify>
  <done>Analyzer returns structured unmet needs, trends, sentiment from Reddit data</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `npm run build` succeeds without errors
- [ ] Python crawler tests pass
- [ ] FastAPI endpoint responds correctly
- [ ] AI analyzer produces valid structured output
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No TypeScript errors
- End-to-end: crawl â†’ analyze produces meaningful results
</success_criteria>

<output>
After completion, create `.planning/phases/01-reddit-deep-dive/02-01-SUMMARY.md`
</output>
